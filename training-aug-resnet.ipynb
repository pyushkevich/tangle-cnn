{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.0.0\n",
      "Torchvision Version:  0.2.1\n",
      "CUDA status:  True\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "print(\"CUDA status: \", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = \"/home/pyushkevich/data/twoway\"\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"resnet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 2\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 16\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 200\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet50\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet50(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /home/pyushkevich/.torch/models/resnet50-19c8e357.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n",
      "Device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomRotation(45),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t conv1.weight\n",
      "\t bn1.weight\n",
      "\t bn1.bias\n",
      "\t layer1.0.conv1.weight\n",
      "\t layer1.0.bn1.weight\n",
      "\t layer1.0.bn1.bias\n",
      "\t layer1.0.conv2.weight\n",
      "\t layer1.0.bn2.weight\n",
      "\t layer1.0.bn2.bias\n",
      "\t layer1.0.conv3.weight\n",
      "\t layer1.0.bn3.weight\n",
      "\t layer1.0.bn3.bias\n",
      "\t layer1.0.downsample.0.weight\n",
      "\t layer1.0.downsample.1.weight\n",
      "\t layer1.0.downsample.1.bias\n",
      "\t layer1.1.conv1.weight\n",
      "\t layer1.1.bn1.weight\n",
      "\t layer1.1.bn1.bias\n",
      "\t layer1.1.conv2.weight\n",
      "\t layer1.1.bn2.weight\n",
      "\t layer1.1.bn2.bias\n",
      "\t layer1.1.conv3.weight\n",
      "\t layer1.1.bn3.weight\n",
      "\t layer1.1.bn3.bias\n",
      "\t layer1.2.conv1.weight\n",
      "\t layer1.2.bn1.weight\n",
      "\t layer1.2.bn1.bias\n",
      "\t layer1.2.conv2.weight\n",
      "\t layer1.2.bn2.weight\n",
      "\t layer1.2.bn2.bias\n",
      "\t layer1.2.conv3.weight\n",
      "\t layer1.2.bn3.weight\n",
      "\t layer1.2.bn3.bias\n",
      "\t layer2.0.conv1.weight\n",
      "\t layer2.0.bn1.weight\n",
      "\t layer2.0.bn1.bias\n",
      "\t layer2.0.conv2.weight\n",
      "\t layer2.0.bn2.weight\n",
      "\t layer2.0.bn2.bias\n",
      "\t layer2.0.conv3.weight\n",
      "\t layer2.0.bn3.weight\n",
      "\t layer2.0.bn3.bias\n",
      "\t layer2.0.downsample.0.weight\n",
      "\t layer2.0.downsample.1.weight\n",
      "\t layer2.0.downsample.1.bias\n",
      "\t layer2.1.conv1.weight\n",
      "\t layer2.1.bn1.weight\n",
      "\t layer2.1.bn1.bias\n",
      "\t layer2.1.conv2.weight\n",
      "\t layer2.1.bn2.weight\n",
      "\t layer2.1.bn2.bias\n",
      "\t layer2.1.conv3.weight\n",
      "\t layer2.1.bn3.weight\n",
      "\t layer2.1.bn3.bias\n",
      "\t layer2.2.conv1.weight\n",
      "\t layer2.2.bn1.weight\n",
      "\t layer2.2.bn1.bias\n",
      "\t layer2.2.conv2.weight\n",
      "\t layer2.2.bn2.weight\n",
      "\t layer2.2.bn2.bias\n",
      "\t layer2.2.conv3.weight\n",
      "\t layer2.2.bn3.weight\n",
      "\t layer2.2.bn3.bias\n",
      "\t layer2.3.conv1.weight\n",
      "\t layer2.3.bn1.weight\n",
      "\t layer2.3.bn1.bias\n",
      "\t layer2.3.conv2.weight\n",
      "\t layer2.3.bn2.weight\n",
      "\t layer2.3.bn2.bias\n",
      "\t layer2.3.conv3.weight\n",
      "\t layer2.3.bn3.weight\n",
      "\t layer2.3.bn3.bias\n",
      "\t layer3.0.conv1.weight\n",
      "\t layer3.0.bn1.weight\n",
      "\t layer3.0.bn1.bias\n",
      "\t layer3.0.conv2.weight\n",
      "\t layer3.0.bn2.weight\n",
      "\t layer3.0.bn2.bias\n",
      "\t layer3.0.conv3.weight\n",
      "\t layer3.0.bn3.weight\n",
      "\t layer3.0.bn3.bias\n",
      "\t layer3.0.downsample.0.weight\n",
      "\t layer3.0.downsample.1.weight\n",
      "\t layer3.0.downsample.1.bias\n",
      "\t layer3.1.conv1.weight\n",
      "\t layer3.1.bn1.weight\n",
      "\t layer3.1.bn1.bias\n",
      "\t layer3.1.conv2.weight\n",
      "\t layer3.1.bn2.weight\n",
      "\t layer3.1.bn2.bias\n",
      "\t layer3.1.conv3.weight\n",
      "\t layer3.1.bn3.weight\n",
      "\t layer3.1.bn3.bias\n",
      "\t layer3.2.conv1.weight\n",
      "\t layer3.2.bn1.weight\n",
      "\t layer3.2.bn1.bias\n",
      "\t layer3.2.conv2.weight\n",
      "\t layer3.2.bn2.weight\n",
      "\t layer3.2.bn2.bias\n",
      "\t layer3.2.conv3.weight\n",
      "\t layer3.2.bn3.weight\n",
      "\t layer3.2.bn3.bias\n",
      "\t layer3.3.conv1.weight\n",
      "\t layer3.3.bn1.weight\n",
      "\t layer3.3.bn1.bias\n",
      "\t layer3.3.conv2.weight\n",
      "\t layer3.3.bn2.weight\n",
      "\t layer3.3.bn2.bias\n",
      "\t layer3.3.conv3.weight\n",
      "\t layer3.3.bn3.weight\n",
      "\t layer3.3.bn3.bias\n",
      "\t layer3.4.conv1.weight\n",
      "\t layer3.4.bn1.weight\n",
      "\t layer3.4.bn1.bias\n",
      "\t layer3.4.conv2.weight\n",
      "\t layer3.4.bn2.weight\n",
      "\t layer3.4.bn2.bias\n",
      "\t layer3.4.conv3.weight\n",
      "\t layer3.4.bn3.weight\n",
      "\t layer3.4.bn3.bias\n",
      "\t layer3.5.conv1.weight\n",
      "\t layer3.5.bn1.weight\n",
      "\t layer3.5.bn1.bias\n",
      "\t layer3.5.conv2.weight\n",
      "\t layer3.5.bn2.weight\n",
      "\t layer3.5.bn2.bias\n",
      "\t layer3.5.conv3.weight\n",
      "\t layer3.5.bn3.weight\n",
      "\t layer3.5.bn3.bias\n",
      "\t layer4.0.conv1.weight\n",
      "\t layer4.0.bn1.weight\n",
      "\t layer4.0.bn1.bias\n",
      "\t layer4.0.conv2.weight\n",
      "\t layer4.0.bn2.weight\n",
      "\t layer4.0.bn2.bias\n",
      "\t layer4.0.conv3.weight\n",
      "\t layer4.0.bn3.weight\n",
      "\t layer4.0.bn3.bias\n",
      "\t layer4.0.downsample.0.weight\n",
      "\t layer4.0.downsample.1.weight\n",
      "\t layer4.0.downsample.1.bias\n",
      "\t layer4.1.conv1.weight\n",
      "\t layer4.1.bn1.weight\n",
      "\t layer4.1.bn1.bias\n",
      "\t layer4.1.conv2.weight\n",
      "\t layer4.1.bn2.weight\n",
      "\t layer4.1.bn2.bias\n",
      "\t layer4.1.conv3.weight\n",
      "\t layer4.1.bn3.weight\n",
      "\t layer4.1.bn3.bias\n",
      "\t layer4.2.conv1.weight\n",
      "\t layer4.2.bn1.weight\n",
      "\t layer4.2.bn1.bias\n",
      "\t layer4.2.conv2.weight\n",
      "\t layer4.2.bn2.weight\n",
      "\t layer4.2.bn2.bias\n",
      "\t layer4.2.conv3.weight\n",
      "\t layer4.2.bn3.weight\n",
      "\t layer4.2.bn3.bias\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "            \n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 0.4049 Acc: 0.8174\n",
      "val Loss: 0.1953 Acc: 0.9525\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 0.2586 Acc: 0.9030\n",
      "val Loss: 0.1849 Acc: 0.9525\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 0.2235 Acc: 0.9183\n",
      "val Loss: 0.2334 Acc: 0.9426\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 0.2451 Acc: 0.9123\n",
      "val Loss: 0.2279 Acc: 0.9393\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 0.2224 Acc: 0.9216\n",
      "val Loss: 0.2126 Acc: 0.9410\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 0.2157 Acc: 0.9123\n",
      "val Loss: 0.2100 Acc: 0.9492\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 0.2145 Acc: 0.9221\n",
      "val Loss: 0.1910 Acc: 0.9426\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 0.1891 Acc: 0.9298\n",
      "val Loss: 0.2196 Acc: 0.9443\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 0.1916 Acc: 0.9282\n",
      "val Loss: 0.1962 Acc: 0.9426\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 0.1847 Acc: 0.9315\n",
      "val Loss: 0.2189 Acc: 0.9377\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 0.1606 Acc: 0.9359\n",
      "val Loss: 0.2172 Acc: 0.9295\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 0.1734 Acc: 0.9364\n",
      "val Loss: 0.2005 Acc: 0.9443\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 0.1609 Acc: 0.9397\n",
      "val Loss: 0.2247 Acc: 0.9508\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 0.1701 Acc: 0.9408\n",
      "val Loss: 0.4509 Acc: 0.8951\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 0.1876 Acc: 0.9287\n",
      "val Loss: 0.2052 Acc: 0.9410\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 0.1571 Acc: 0.9419\n",
      "val Loss: 0.2186 Acc: 0.9393\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 0.1517 Acc: 0.9446\n",
      "val Loss: 0.2020 Acc: 0.9508\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 0.1529 Acc: 0.9419\n",
      "val Loss: 0.1938 Acc: 0.9525\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 0.1719 Acc: 0.9320\n",
      "val Loss: 0.2243 Acc: 0.9459\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 0.1380 Acc: 0.9479\n",
      "val Loss: 0.2253 Acc: 0.9426\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 0.1500 Acc: 0.9474\n",
      "val Loss: 0.2323 Acc: 0.9492\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 0.1529 Acc: 0.9435\n",
      "val Loss: 0.1842 Acc: 0.9508\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 0.1554 Acc: 0.9424\n",
      "val Loss: 0.2375 Acc: 0.9508\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 0.1637 Acc: 0.9408\n",
      "val Loss: 0.1809 Acc: 0.9508\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 0.1376 Acc: 0.9457\n",
      "val Loss: 0.2139 Acc: 0.9443\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 0.1531 Acc: 0.9457\n",
      "val Loss: 0.2112 Acc: 0.9361\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 0.1444 Acc: 0.9408\n",
      "val Loss: 0.2503 Acc: 0.9508\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 0.1506 Acc: 0.9485\n",
      "val Loss: 0.3489 Acc: 0.9295\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 0.1430 Acc: 0.9424\n",
      "val Loss: 0.1888 Acc: 0.9525\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 0.1091 Acc: 0.9534\n",
      "val Loss: 0.2419 Acc: 0.9525\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 0.1225 Acc: 0.9518\n",
      "val Loss: 0.2891 Acc: 0.9410\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 0.1343 Acc: 0.9424\n",
      "val Loss: 0.2984 Acc: 0.9410\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 0.1124 Acc: 0.9523\n",
      "val Loss: 0.2281 Acc: 0.9492\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 0.1459 Acc: 0.9468\n",
      "val Loss: 0.2076 Acc: 0.9525\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 0.1211 Acc: 0.9512\n",
      "val Loss: 0.2200 Acc: 0.9557\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 0.1330 Acc: 0.9446\n",
      "val Loss: 0.1888 Acc: 0.9574\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 0.1106 Acc: 0.9589\n",
      "val Loss: 0.2095 Acc: 0.9574\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 0.1122 Acc: 0.9534\n",
      "val Loss: 0.2505 Acc: 0.9311\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 0.1160 Acc: 0.9518\n",
      "val Loss: 0.2256 Acc: 0.9393\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 0.1286 Acc: 0.9512\n",
      "val Loss: 0.1999 Acc: 0.9541\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 0.1174 Acc: 0.9501\n",
      "val Loss: 0.1898 Acc: 0.9557\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 0.1049 Acc: 0.9633\n",
      "val Loss: 0.2290 Acc: 0.9393\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 0.1276 Acc: 0.9452\n",
      "val Loss: 0.2890 Acc: 0.9295\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 0.1081 Acc: 0.9594\n",
      "val Loss: 0.2332 Acc: 0.9508\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 0.1260 Acc: 0.9501\n",
      "val Loss: 0.2986 Acc: 0.9426\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 0.1181 Acc: 0.9529\n",
      "val Loss: 0.2776 Acc: 0.9426\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 0.0985 Acc: 0.9622\n",
      "val Loss: 0.2240 Acc: 0.9557\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 0.0995 Acc: 0.9594\n",
      "val Loss: 0.2656 Acc: 0.9557\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 0.1104 Acc: 0.9572\n",
      "val Loss: 0.2305 Acc: 0.9508\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 0.1101 Acc: 0.9583\n",
      "val Loss: 0.2631 Acc: 0.9508\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 0.1218 Acc: 0.9556\n",
      "val Loss: 0.2491 Acc: 0.9525\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 0.0955 Acc: 0.9594\n",
      "val Loss: 0.2983 Acc: 0.9426\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 0.1102 Acc: 0.9627\n",
      "val Loss: 0.2265 Acc: 0.9508\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 0.1103 Acc: 0.9501\n",
      "val Loss: 0.2645 Acc: 0.9328\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 0.1067 Acc: 0.9611\n",
      "val Loss: 0.2838 Acc: 0.9148\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 0.1139 Acc: 0.9550\n",
      "val Loss: 0.2403 Acc: 0.9590\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 0.1052 Acc: 0.9600\n",
      "val Loss: 0.2139 Acc: 0.9508\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 0.1111 Acc: 0.9539\n",
      "val Loss: 0.1885 Acc: 0.9541\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 0.1150 Acc: 0.9534\n",
      "val Loss: 0.2284 Acc: 0.9492\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 0.1066 Acc: 0.9600\n",
      "val Loss: 0.2409 Acc: 0.9525\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 0.1008 Acc: 0.9600\n",
      "val Loss: 0.2778 Acc: 0.9574\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 0.0843 Acc: 0.9627\n",
      "val Loss: 0.2069 Acc: 0.9574\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 0.1038 Acc: 0.9633\n",
      "val Loss: 0.3704 Acc: 0.9443\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 0.0866 Acc: 0.9704\n",
      "val Loss: 0.2712 Acc: 0.9492\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 0.1078 Acc: 0.9583\n",
      "val Loss: 0.2894 Acc: 0.9393\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 0.0964 Acc: 0.9600\n",
      "val Loss: 0.2176 Acc: 0.9574\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 0.0941 Acc: 0.9644\n",
      "val Loss: 0.2464 Acc: 0.9541\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 0.0965 Acc: 0.9594\n",
      "val Loss: 0.3398 Acc: 0.9557\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 0.1032 Acc: 0.9561\n",
      "val Loss: 0.3380 Acc: 0.9377\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 0.1050 Acc: 0.9583\n",
      "val Loss: 0.2847 Acc: 0.9525\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 0.0864 Acc: 0.9633\n",
      "val Loss: 0.2695 Acc: 0.9426\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 0.0963 Acc: 0.9660\n",
      "val Loss: 0.2665 Acc: 0.9443\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 0.0898 Acc: 0.9605\n",
      "val Loss: 0.2627 Acc: 0.9508\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 0.0941 Acc: 0.9682\n",
      "val Loss: 0.2211 Acc: 0.9525\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 0.0875 Acc: 0.9616\n",
      "val Loss: 0.2050 Acc: 0.9574\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 0.0987 Acc: 0.9600\n",
      "val Loss: 0.3077 Acc: 0.9525\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 0.0932 Acc: 0.9633\n",
      "val Loss: 0.2294 Acc: 0.9492\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 0.0880 Acc: 0.9622\n",
      "val Loss: 0.3477 Acc: 0.9459\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 0.0773 Acc: 0.9671\n",
      "val Loss: 0.3452 Acc: 0.9410\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 0.0902 Acc: 0.9644\n",
      "val Loss: 0.2876 Acc: 0.9508\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 0.0831 Acc: 0.9682\n",
      "val Loss: 0.2774 Acc: 0.9492\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 0.0808 Acc: 0.9693\n",
      "val Loss: 0.3017 Acc: 0.9393\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 0.0773 Acc: 0.9677\n",
      "val Loss: 0.2466 Acc: 0.9508\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 0.0822 Acc: 0.9666\n",
      "val Loss: 0.3040 Acc: 0.9459\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 0.0861 Acc: 0.9655\n",
      "val Loss: 0.2496 Acc: 0.9508\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 0.0940 Acc: 0.9627\n",
      "val Loss: 0.2927 Acc: 0.9459\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 0.0726 Acc: 0.9682\n",
      "val Loss: 0.3130 Acc: 0.9492\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 0.0833 Acc: 0.9671\n",
      "val Loss: 0.3251 Acc: 0.9377\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 0.0828 Acc: 0.9688\n",
      "val Loss: 0.3116 Acc: 0.9459\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 0.0845 Acc: 0.9660\n",
      "val Loss: 0.3295 Acc: 0.9377\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 0.0927 Acc: 0.9688\n",
      "val Loss: 0.3315 Acc: 0.9426\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 0.0919 Acc: 0.9666\n",
      "val Loss: 0.3155 Acc: 0.9230\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 0.0993 Acc: 0.9616\n",
      "val Loss: 0.2939 Acc: 0.9508\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 0.1042 Acc: 0.9622\n",
      "val Loss: 0.2944 Acc: 0.9410\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 0.0804 Acc: 0.9671\n",
      "val Loss: 0.2408 Acc: 0.9459\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 0.0889 Acc: 0.9644\n",
      "val Loss: 0.2639 Acc: 0.9393\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 0.0839 Acc: 0.9638\n",
      "val Loss: 0.3017 Acc: 0.9475\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 0.1001 Acc: 0.9589\n",
      "val Loss: 0.2945 Acc: 0.9393\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 0.0927 Acc: 0.9649\n",
      "val Loss: 0.3901 Acc: 0.9049\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 0.1013 Acc: 0.9660\n",
      "val Loss: 0.2300 Acc: 0.9262\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 0.0702 Acc: 0.9698\n",
      "val Loss: 0.2474 Acc: 0.9508\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 0.0952 Acc: 0.9611\n",
      "val Loss: 0.3344 Acc: 0.9459\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 0.0793 Acc: 0.9660\n",
      "val Loss: 0.3561 Acc: 0.9213\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 0.0866 Acc: 0.9688\n",
      "val Loss: 0.3257 Acc: 0.9508\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 0.0865 Acc: 0.9655\n",
      "val Loss: 0.2595 Acc: 0.9459\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 0.1010 Acc: 0.9616\n",
      "val Loss: 0.2497 Acc: 0.9475\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 0.0792 Acc: 0.9682\n",
      "val Loss: 0.2275 Acc: 0.9557\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 0.0805 Acc: 0.9671\n",
      "val Loss: 0.2384 Acc: 0.9541\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 0.0688 Acc: 0.9726\n",
      "val Loss: 0.2514 Acc: 0.9557\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 0.0924 Acc: 0.9633\n",
      "val Loss: 0.2574 Acc: 0.9508\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 0.0947 Acc: 0.9589\n",
      "val Loss: 0.3078 Acc: 0.9557\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 0.0886 Acc: 0.9682\n",
      "val Loss: 0.2777 Acc: 0.9541\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 0.0894 Acc: 0.9666\n",
      "val Loss: 0.2429 Acc: 0.9508\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 0.0820 Acc: 0.9666\n",
      "val Loss: 0.2636 Acc: 0.9541\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 0.0768 Acc: 0.9671\n",
      "val Loss: 0.3086 Acc: 0.9410\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 0.0864 Acc: 0.9644\n",
      "val Loss: 0.2327 Acc: 0.9410\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 0.0772 Acc: 0.9682\n",
      "val Loss: 0.3598 Acc: 0.9410\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 0.0827 Acc: 0.9677\n",
      "val Loss: 0.2339 Acc: 0.9475\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 0.0964 Acc: 0.9644\n",
      "val Loss: 0.3229 Acc: 0.9082\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 0.0754 Acc: 0.9726\n",
      "val Loss: 0.2558 Acc: 0.9443\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 0.0815 Acc: 0.9671\n",
      "val Loss: 0.2386 Acc: 0.9590\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 0.0643 Acc: 0.9737\n",
      "val Loss: 0.2445 Acc: 0.9443\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 0.0803 Acc: 0.9682\n",
      "val Loss: 0.2777 Acc: 0.9393\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 0.0715 Acc: 0.9704\n",
      "val Loss: 0.2358 Acc: 0.9492\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 0.0921 Acc: 0.9638\n",
      "val Loss: 0.2353 Acc: 0.9541\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 0.0843 Acc: 0.9649\n",
      "val Loss: 0.3347 Acc: 0.9295\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 0.0832 Acc: 0.9682\n",
      "val Loss: 0.2896 Acc: 0.9475\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 0.0734 Acc: 0.9720\n",
      "val Loss: 0.3322 Acc: 0.9393\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 0.0813 Acc: 0.9666\n",
      "val Loss: 0.3040 Acc: 0.9525\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 0.0753 Acc: 0.9677\n",
      "val Loss: 0.2820 Acc: 0.9492\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 0.0850 Acc: 0.9660\n",
      "val Loss: 0.3936 Acc: 0.9426\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 0.0793 Acc: 0.9709\n",
      "val Loss: 0.3888 Acc: 0.9328\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 0.0824 Acc: 0.9671\n",
      "val Loss: 0.2775 Acc: 0.9393\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 0.0645 Acc: 0.9737\n",
      "val Loss: 0.3185 Acc: 0.9295\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 0.0690 Acc: 0.9748\n",
      "val Loss: 0.2911 Acc: 0.9541\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 0.0742 Acc: 0.9698\n",
      "val Loss: 0.2664 Acc: 0.9492\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 0.0949 Acc: 0.9660\n",
      "val Loss: 0.3571 Acc: 0.9344\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 0.0821 Acc: 0.9644\n",
      "val Loss: 0.2387 Acc: 0.9590\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 0.0652 Acc: 0.9720\n",
      "val Loss: 0.3821 Acc: 0.9213\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 0.0750 Acc: 0.9655\n",
      "val Loss: 0.3160 Acc: 0.9311\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 0.0757 Acc: 0.9709\n",
      "val Loss: 0.3029 Acc: 0.9475\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 0.0829 Acc: 0.9682\n",
      "val Loss: 0.2826 Acc: 0.9459\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 0.0603 Acc: 0.9720\n",
      "val Loss: 0.3179 Acc: 0.9492\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 0.0772 Acc: 0.9693\n",
      "val Loss: 0.3713 Acc: 0.9426\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 0.0752 Acc: 0.9704\n",
      "val Loss: 0.3428 Acc: 0.9459\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 0.0768 Acc: 0.9704\n",
      "val Loss: 0.4013 Acc: 0.9311\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 0.0759 Acc: 0.9715\n",
      "val Loss: 0.3834 Acc: 0.9410\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 0.0831 Acc: 0.9720\n",
      "val Loss: 0.2688 Acc: 0.9475\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 0.0893 Acc: 0.9638\n",
      "val Loss: 0.2546 Acc: 0.9426\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 0.0628 Acc: 0.9759\n",
      "val Loss: 0.2589 Acc: 0.9492\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 0.0848 Acc: 0.9655\n",
      "val Loss: 0.2138 Acc: 0.9459\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 0.0607 Acc: 0.9731\n",
      "val Loss: 0.2709 Acc: 0.9459\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 0.0746 Acc: 0.9704\n",
      "val Loss: 0.2523 Acc: 0.9557\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 0.0676 Acc: 0.9715\n",
      "val Loss: 0.3122 Acc: 0.9426\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 0.0667 Acc: 0.9731\n",
      "val Loss: 0.2892 Acc: 0.9525\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 0.0586 Acc: 0.9715\n",
      "val Loss: 0.3191 Acc: 0.9574\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 0.0614 Acc: 0.9693\n",
      "val Loss: 0.4338 Acc: 0.9492\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 0.0504 Acc: 0.9759\n",
      "val Loss: 0.3452 Acc: 0.9525\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 0.0721 Acc: 0.9715\n",
      "val Loss: 0.2412 Acc: 0.9590\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 0.0675 Acc: 0.9759\n",
      "val Loss: 0.3211 Acc: 0.9508\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 0.0644 Acc: 0.9726\n",
      "val Loss: 0.3219 Acc: 0.9443\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 0.0634 Acc: 0.9742\n",
      "val Loss: 0.3340 Acc: 0.9475\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 0.0686 Acc: 0.9726\n",
      "val Loss: 0.3950 Acc: 0.9459\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 0.0624 Acc: 0.9742\n",
      "val Loss: 0.4068 Acc: 0.9459\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 0.0809 Acc: 0.9655\n",
      "val Loss: 0.4702 Acc: 0.9230\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 0.0657 Acc: 0.9720\n",
      "val Loss: 0.2648 Acc: 0.9525\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 0.0704 Acc: 0.9693\n",
      "val Loss: 0.2634 Acc: 0.9508\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 0.0724 Acc: 0.9715\n",
      "val Loss: 0.2861 Acc: 0.9475\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 0.0570 Acc: 0.9731\n",
      "val Loss: 0.3120 Acc: 0.9443\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 0.0714 Acc: 0.9726\n",
      "val Loss: 0.4229 Acc: 0.9197\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 0.0848 Acc: 0.9655\n",
      "val Loss: 0.2934 Acc: 0.9344\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 0.0730 Acc: 0.9720\n",
      "val Loss: 0.2527 Acc: 0.9443\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 0.0634 Acc: 0.9715\n",
      "val Loss: 0.2101 Acc: 0.9525\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 0.0664 Acc: 0.9759\n",
      "val Loss: 0.2634 Acc: 0.9525\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 0.0752 Acc: 0.9677\n",
      "val Loss: 0.2554 Acc: 0.9525\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 0.0633 Acc: 0.9759\n",
      "val Loss: 0.2490 Acc: 0.9525\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 0.0531 Acc: 0.9770\n",
      "val Loss: 0.2712 Acc: 0.9525\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 0.0597 Acc: 0.9764\n",
      "val Loss: 0.2339 Acc: 0.9574\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 0.0707 Acc: 0.9682\n",
      "val Loss: 0.2920 Acc: 0.9459\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 0.0584 Acc: 0.9742\n",
      "val Loss: 0.2571 Acc: 0.9623\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 0.0764 Acc: 0.9671\n",
      "val Loss: 0.2951 Acc: 0.9393\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 0.0728 Acc: 0.9677\n",
      "val Loss: 0.3413 Acc: 0.9328\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 0.0623 Acc: 0.9764\n",
      "val Loss: 0.3245 Acc: 0.9525\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 0.0573 Acc: 0.9770\n",
      "val Loss: 0.3098 Acc: 0.9492\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 0.0556 Acc: 0.9792\n",
      "val Loss: 0.2794 Acc: 0.9557\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 0.0551 Acc: 0.9770\n",
      "val Loss: 0.2637 Acc: 0.9541\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 0.0825 Acc: 0.9704\n",
      "val Loss: 0.3058 Acc: 0.9492\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 0.0594 Acc: 0.9753\n",
      "val Loss: 0.2472 Acc: 0.9426\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 0.0627 Acc: 0.9720\n",
      "val Loss: 0.2462 Acc: 0.9475\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 0.0615 Acc: 0.9781\n",
      "val Loss: 0.2177 Acc: 0.9557\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 0.0649 Acc: 0.9726\n",
      "val Loss: 0.2360 Acc: 0.9623\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 0.0687 Acc: 0.9693\n",
      "val Loss: 0.2451 Acc: 0.9541\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n",
      "train Loss: 0.0517 Acc: 0.9786\n",
      "val Loss: 0.2396 Acc: 0.9574\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 0.0563 Acc: 0.9775\n",
      "val Loss: 0.2743 Acc: 0.9508\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 0.0600 Acc: 0.9715\n",
      "val Loss: 0.2437 Acc: 0.9590\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 0.0507 Acc: 0.9814\n",
      "val Loss: 0.2487 Acc: 0.9656\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 0.0590 Acc: 0.9781\n",
      "val Loss: 0.2118 Acc: 0.9541\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 0.0672 Acc: 0.9720\n",
      "val Loss: 0.2692 Acc: 0.9508\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 0.0859 Acc: 0.9715\n",
      "val Loss: 0.3075 Acc: 0.9246\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 0.0603 Acc: 0.9759\n",
      "val Loss: 0.2234 Acc: 0.9525\n",
      "\n",
      "Training complete in 79m 34s\n",
      "Best val Acc: 0.965574\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(),\"/home/pyushkevich/resnet/my_resnet_50_200epoch.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset=datasets.ImageFolder(os.path.join(data_dir, \"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, labels in test_dataset:\n",
    "    model_ft.eval()\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model_ft(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
